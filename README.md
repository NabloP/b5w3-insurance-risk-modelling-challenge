# B5W3: Insurance Risk Analytics & Predictive Modeling Week 3 - 10 Academy

## ğŸ—‚ Challenge Context
This repository documents the submission for 10 Academyâ€™s **B5W3: Insurance Risk Analytics & Predictive Modeling** challenge.
The goal is to support AlphaCare Insurance Solutions (ACIS) in optimizing underwriting and pricing by analyzing customer, vehicle, and claims data to:

- Identify low-risk customer segments

- Predict future risk exposure

- Enable data-driven premium optimization

This project simulates the role of a risk analyst at AlphaCare Insurance Solutions (ACIS), supporting actuarial and underwriting teams with data-driven insights for optimizing premium pricing and minimizing claims exposure.

The project includes:

- ğŸ§¹ Clean and structured ingestion of raw customer, vehicle, and claims datasets

- ğŸ“Š Multi-layered Exploratory Data Analysis (EDA) across customer, product, geographic, and vehicle dimensions

- ğŸ§  Modular profiling of loss ratio, outliers, and segment-specific profitability

- ğŸ—ƒï¸ Defensive schema auditing and data quality validation routines

- ğŸ“¦ Reproducible data versioning using DVC with Git and local cache integration

- ğŸ§ª Scaffolded modeling pipeline for classification-based claims risk prediction (planned)

- âœ… Structured orchestration of insights through testable, class-based Python modules and `eda_orchestrator.py` runner script


## ğŸ”§ Project Setup

To reproduce this environment:

1. Clone the repository:

```bash
git clone https://github.com/NabloP/b5w3-insurance-risk-modelling-challenge.git
cd b5w3-insurance-risk-modelling-challenge
```

2. Create and activate a virtual environment:
   
**On Windows:**
    
```bash
python -m venv insurance-challenge
.\insurance-challenge\Scripts\Activate.ps1
```

**On macOS/Linux:**

```bash
python3 -m venv insurance-challenge
source insurance-challenge/bin/activate
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

## âš™ï¸ CI/CD (GitHub Actions)

This project uses GitHub Actions for Continuous Integration. On every `push` or `pull_request` event, the following workflow is triggered:

- Checkout repo

- Set up Python 3.10

- Install dependencies from `requirements.txt`

CI workflow is defined at:

    `.github/workflows/unittests.yml`

## ğŸ“ Project Structure

<!-- TREE START -->
ğŸ“ Project Structure

solar-challenge-week1/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ unittests.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â”œâ”€â”€ loss_ratio_bubble_map.png
â”‚   â”‚   â””â”€â”€ plots/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ MachineLearningRating_v3.txt.dvc
â”‚       â”œâ”€â”€ opendb-2025-06-17.csv.dvc
â”œâ”€â”€ docs/
â”œâ”€â”€ models/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ task-1-eda-statistical-planning.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ eda_orchestrator.py
â”‚   â”œâ”€â”€ generate_tree.py
â”‚   â”œâ”€â”€ version_datasets.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â””â”€â”€ eda/
â”‚       â”œâ”€â”€ defensive_schema_auditor.py
â”‚       â”œâ”€â”€ distribution_analyzer.py
â”‚       â”œâ”€â”€ gender_risk_profiler.py
â”‚       â”œâ”€â”€ geo_risk_visualizer.py
â”‚       â”œâ”€â”€ iqr_outlier_detector.py
â”‚       â”œâ”€â”€ numeric_plotter.py
â”‚       â”œâ”€â”€ plan_feature_risk_profiler.py
â”‚       â”œâ”€â”€ schema_auditor.py
â”‚       â”œâ”€â”€ schema_guardrails.py
â”‚       â”œâ”€â”€ temporal_analyzer.py
â”‚       â”œâ”€â”€ vehicle_risk_profiler.py
â”œâ”€â”€ tests/
â””â”€â”€ ui/
<!-- TREE END -->


## âœ… Status

- â˜‘ï¸ Task 1 complete: Full EDA pipeline implemented across 10 modular risk layers (loss ratio, outliers, geo, schema, etc.)

- â˜‘ï¸ Task 2 complete: DVC tracking initialized with Git integration, local remote configured, and raw datasets versioned

- ğŸ—ï¸ Task 3 scaffolded: Modeling modules prepared for claims classification and segment-level risk prediction

- ğŸ—ï¸ Task 4 scaffolded: Feature engineering and pricing optimization logic designed (implementation upcoming)

â˜‘ï¸ Project architecture: Fully modular `src/`, `scripts/`, and `notebooks/` structure with reproducible orchestration via `eda_orchestrator.py` and `v`ersion_datasets.py`


## ğŸ“¦ What's in This Repo

This repository is structured to maximize modularity, reusability, and clarity:

- ğŸ“ Layered Python module structure for risk profiling (src/eda/), geographic mapping (src/geo/), and schema auditing (src/)

- ğŸ§ª CI-ready architecture using GitHub Actions for reproducible tests via pytest

- ğŸ“¦ DVC integration for versioned tracking of raw and processed datasets (with local remote and cache routing)

- ğŸ§¹ Clean orchestration scripts (eda_orchestrator.py, version_datasets.py) for Task 1â€“2 reproducibility

- ğŸ§  Risk analysis modules written with defensive programming, strong validation, and class-based design

- ğŸ“Š Insightful plots (loss ratio heatmaps, bar charts, outlier maps) auto-rendered via orchestrator pipeline

- ğŸ§¾ Consistent Git hygiene with .gitignore, no committed .csv or .venv, and modular commit history

- ğŸ§ª Notebook-first development approach supported by CLI runners and reusable core modules


- ğŸ§  **My Contributions:** All project scaffolding, README setup, automation scripts, and CI configuration were done from scratch by me


## ğŸ” DVC Configuration & Versioning (Task 2)
This project uses **Data Version Control (DVC)** to ensure auditable and reproducible handling of insurance datasets across all preprocessing stages.

### âœ… Versioned Artifacts
The following DVC artifacts are tracked and committed to the repository:

File	| Purpose
--------|---------
data/raw/MachineLearningRating_v3.txt.dvc	| Tracks raw dataset (customer + claims)
data/raw/opendb-2025-06-17.csv.dvc	| Tracks auxiliary postal code metadata
.dvc/config	| Stores remote and cache settings
.gitignore	| Automatically updated to ignore large .csv files

Note: This project currently uses .dvc-style tracking (per file), not dvc.yaml pipelines. The dvc.yaml file will be added in Task 3â€“4 for full ML pipeline definition.

### ğŸ“¦ DVC Remote Configuration
DVC is configured to use a local remote directory (outside the Git repo) for safe, decoupled storage:

```swift

Remote path: C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache
```

This is specified in .dvc/config as:

```ini
['cache']
    dir = C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache
```

And confirmed via:

```bash
dvc config cache.dir "C:/Users/admin/Documents/GIT Repositories/dvc_remote/.dvc/cache"
```

### ğŸ” How to Push to DVC Remote

To sync all .dvc-tracked data to the configured local remote:

```bash
dvc add data/raw/MachineLearningRating_v3.txt
dvc add data/raw/opendb-2025-06-17.csv
git add data/raw/*.dvc .gitignore
git commit -m "Track raw datasets with DVC"
dvc push
```

### ğŸ”§ Automation Support

For reproducibility, all versioning steps can be automated using:

```bash
python scripts/version_datasets.py
```

This script:

- Adds tracked datasets to DVC

- Commits .dvc files to Git

- Pushes artifacts to remote

- Logs all actions to dvc_logs/




## ğŸ§ª Usage

### ğŸ›ï¸ Option 1: Using the Streamlit App

The Streamlit UI (`ui/app_streamlit.py`) provides an interactive interface to perform both review scraping and cleaning with no code required.

**To launch the app locally:**
```bash
streamlit run ui/app_streamlit.py
```

**ğŸ§© Streamlit Features:**

- Scrape Google Play reviews for CBE, BOA, or Dashen

- Export reviews per-bank or as a combined dataset

- Preview scraped files in-app

- Clean any raw file from `data/raw/`

- View sidebar diagnostics for:

    - Missing fields dropped

    - Blank reviews removed

    - Duplicate `reviewIds` filtered

- Download cleaned outputs directly

All exports are timestamped and saved to `data/raw/` or `data/cleaned/` depending on context.


### ğŸ Option 2: Using Python Scripts
For automated, reproducible runs from the command line or notebooks, use the modular runners in the `scripts/` folder.

**ğŸ”¹ Scraping Reviews**
To scrape reviews from one or more banks and export to CSV:

```python
 scripts/scraping_runner.py --bank CBE --num_reviews 100
 ```

Options:

- `--bank`: one of `CBE`, `BOA`, `Dashen`, or `all`

- `--num_reviews`: maximum number of reviews per app


**ğŸ”¹ Cleaning Reviews**
To clean a raw file and export the cleaned result:

```python 
scripts/cleaning_runner.py --input_file data/raw/reviews_BOA_20250607_124729.csv
```

This removes:

- Rows with missing fields

- Blank or whitespace-only reviews

- Duplicate entries by `reviewId`

Cleaned files are saved under `data/cleaned/`.

### ğŸ” How It Works Internally

Both the Streamlit app and script-based runners share the same core logic, implemented in the following modules:

- `src/scraper/review_scraper.py` â€“ Fetches reviews from the Play Store

- `src/cleaning/review_cleaner.py` â€“ Cleans and validates reviews

- `src/utils/preprocessing.py` â€“ Shared text preprocessing functions

- `scripts/run_streamlit.py` â€“ Optional wrapper for launching UI from CLI

- `scripts/generate_tree.py` â€“ Auto-generates folder tree for `README.md`

Each module is written using object-oriented principles and is fully reusable across CLI, notebook, and UI contexts.


### ğŸ§  NLP Pipeline + Storage + Visuals

This section summarizes the full enrichment, database, and insights flow from **Task 2**, **Task 3**, and **Task 4**.

---

#### ğŸ”¹ Task 2: Sentiment + Theme Enrichment

Run the full sentiment and theme extraction pipeline with:

```bash
python scripts/sentiment_pipeline.py
```

What it does:

- ğŸ§¹ Loads cleaned reviews from `data/cleaned/`

- ğŸ”¡ Normalizes text using **SymSpell** and **spaCy**

- ğŸ’¬ Applies ensemble sentiment scoring:

    - `VADER`
    - `TextBlob`
    - `DistilBERT` (locally loaded from `models/`)

- ğŸ“Š Assigns each review:

    - `ensemble_sentiment` label (bullish, neutral, bearish)
    - `sentiment_uncertainty` (std deviation of ensemble)
    - `sentiment_mismatch_flag` if BERT disagrees with VADER/TextBlob

ğŸ” Extracts:

- Top **keywords** (via TF-IDF)

- Top **keyphrases** (noun chunking)

- UX **themes** (from curated keywordâ€“theme dictionaries)

ğŸ’¾ Saves outputs to:

- `data/outputs/reviews_enriched_all.csv`

- `data/outputs/reviews_with_sentiment_themes.csv`

âœ… Designed to **auto-run** with no input flags or prompts.

---

### ğŸ›¢ï¸ Task 3: Oracle XE Database Integration (Relational Storage)

The project includes full support for enterprise-grade relational storage using Oracle XE. Reviews are inserted into a normalized schema with integrity constraints.

**Key Features:**

- âœ… `banks` and `reviews` tables in 3NF

- âœ… Modular insert logic via `oracle_insert.py`

- âœ… Connection security via `os.getenv()` (no hardcoded secrets)

- âœ… Full rollback + diagnostic prints on insert failure

- âœ… Schema DDL included for reproducibility

### ğŸ§± Schema Design

1. **`banks` Table**

| Column      | Type        | Description            |
|-------------|-------------|------------------------|
| `bank_id`   | INTEGER PK  | Unique bank identifier |
| `bank_name` | VARCHAR(50) | Name of the bank       |

2. reviews Table

| Column        | Type             | Description                          |
|---------------|------------------|--------------------------------------|
| `review_id`   | VARCHAR2(100) PK | Unique review ID                     |
| `bank_id`     | INTEGER FK       | Foreign key referencing banks        |
| `review_text` | CLOB             | Full text of the review              |
| `rating`      | INTEGER          | App rating (1 to 5)                  |
| `review_date` | DATE             | Parsed date of review                |
| `source`      | VARCHAR2(50)     | Always 'Google Play' in this project |

### ğŸ”Œ Usage

To insert reviews into Oracle XE after enrichment:

```bash
python scripts/oracle_insert.py
```

What it does:

- ğŸ“¦ Loads enriched review file from:

    - `data/outputs/reviews_with_sentiment_themes.csv`

- ğŸ—ï¸ Defines and initializes schema:

    - `reviews` table with fields for rating, sentiment, text, and metadata
    - `themes` table (if normalized)

- ğŸ”— Connects to Oracle XE (uses `src/db/oracle_connector.py`)

- ğŸ§© Inserts cleaned records with:

    - Full error handling
    - Per-row diagnostic feedback if insertions fail

- ğŸ›‘ Can be rerun idempotently (e.g. if schema already exists)

â˜‘ï¸ Safe to run from CLI after enrichment is complete.

ğŸ“ Related Files:

- `scripts/oracle_insert.py`: Insert runner

- `src/db/oracle_connector.py`: Secure DB connection

- `task-3-oracle-storage.ipynb`: Notebook walkthrough of schema + insert test


**ğŸ“„ [Oracle XE Storage Documentation](docs/oracle_storage_overview.md)**

---

### ğŸ“ˆ Task 4: Visualization & KPI Dashboard
Run all Task 4 UX and sentiment visualizations with:

```bash
python scripts/visualize_insights.py
```

What it does:

- ğŸ§  Loads enriched reviews from `data/outputs/reviews_with_sentiment_themes.csv`

- ğŸ“Š Auto-generates plots for:

    - ğŸ“ˆ Average rating per theme per bank (heatmap)
    - ğŸ”¥ Complaint clusters (negative sentiment by theme and bank)
    - â˜ï¸ Word clouds (bank-specific vocabulary)
    - ğŸ› ï¸ Feature request volume (negative themes only)
    - ğŸ’ Bubble chart for theme occurrence vs avg. rating

- ğŸ“ Top-level theme in bubble chart is editable in-code

- ğŸ“‚ Does not auto-save â€” plots are rendered inline

- âœ… Designed for notebook-first and CLI workflows

No flags or CLI inputs required â€” this script auto-runs all diagnostics sequentially.


## ğŸ§  Design Philosophy
This project was developed with a focus on:

- âœ… Modular Python design using classes, helper modules, and runners (clean script folders and testable code)
- âœ… High commenting density to meet AI and human readability expectations
- âœ… Clarity (in folder structure, README, and docstrings)
- âœ… Reproducibility through consistent Git hygiene and generate_tree.py
- âœ… Rubric-alignment (clear deliverables, EDA, and insights)

## ğŸš€ Author
Nabil Mohamed
AIM Bootcamp Participant
GitHub: [NabloP](https://github.com/NabloP)